{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f51544",
   "metadata": {},
   "source": [
    "# Example by Lovkush of how experimentation might look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install transformer_lens -U \"huggingface_hub[cli]\" transformers jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf027ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.utils import get_current_time_str\n",
    "#from src.utils import get_repo_root\n",
    "#import os\n",
    "from tqdm import tqdm\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDevice():\n",
    "    if torch.cuda.is_available(): #nvidia/runpod\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\") #apple silicon\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "DEVICE = getDevice()\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9709ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen1.5-1.8B-Chat into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    }
   ],
   "source": [
    "def get_model(model_name):\n",
    "    # load model from HF and get all the hidden states\n",
    "    model = HookedTransformer.from_pretrained_no_processing(model_name, device = DEVICE, dtype=torch.float16, default_padding_side='left', output_hidden_states=True)\n",
    "    model.eval() #inference mode - no gradients needed\n",
    "    model.to(DEVICE)\n",
    "    return model\n",
    "\n",
    "model = get_model(\"Qwen/Qwen1.5-1.8B-Chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d1758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompt(model: HookedTransformer, prompt_str: str, verbose=False) -> str:\n",
    "    prompt_message = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_str}\n",
    "    ]\n",
    "\n",
    "    if verbose:\n",
    "        print(model.tokenizer.apply_chat_template(\n",
    "            prompt_message,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        ))\n",
    "    prompt_chat_tokenized = model.tokenizer.apply_chat_template(\n",
    "        prompt_message, tokenize=True, add_generation_prompt=True)\n",
    "    prompt_chat_str = model.tokenizer.apply_chat_template(\n",
    "        prompt_message, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt_chat_tokenized, prompt_chat_str\n",
    "\n",
    "def generate_output(model: HookedTransformer, prompt_chat_str: str, max_new_tokens: int) -> tuple[str, dict, int]:\n",
    "    \"\"\"Generate output string, cache, and number of tokens generated.\"\"\"\n",
    "    output_str = prompt_chat_str\n",
    "    for i in tqdm(range(max_new_tokens)):\n",
    "        # Get the logits and cache for the current prompt\n",
    "        logits, cache = model.run_with_cache(output_str)\n",
    "\n",
    "        # Get the predicted next token (using argmax for temperature 0)\n",
    "        next_token = logits[0, -1].argmax()\n",
    "\n",
    "        # Convert the next token to a string\n",
    "        next_token_str = model.to_string(next_token)\n",
    "\n",
    "        # Append the new token to the prompt for the next iteration\n",
    "        output_str += next_token_str\n",
    "        \n",
    "        if next_token.item() == model.tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return output_str, cache, i+1\n",
    "\n",
    "def get_mean_resids_per_layer(model: HookedTransformer, cache: dict, n_tokens_generated: int, n_tokens_input: int) -> list[torch.Tensor]:\n",
    "    mean_resids_per_layer: list[torch.Tensor] = []\n",
    "    n_tokens = n_tokens_generated + n_tokens_input\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        resids_pre = cache[f\"blocks.{layer}.hook_resid_pre\"] # (batch, seq_len, d_model)\n",
    "        assert resids_pre.shape == (1, n_tokens-1, model.cfg.d_model)\n",
    "\n",
    "        # keep only residuals for the generated tokens\n",
    "        resids_pre = resids_pre[:, n_tokens_input:]\n",
    "        assert resids_pre.shape == (1, n_tokens_generated-1, model.cfg.d_model)\n",
    "        \n",
    "        # take the mean across tokens\n",
    "        resids_pre = resids_pre.mean(dim=1, keepdim=True)\n",
    "        assert resids_pre.shape == (1, 1, model.cfg.d_model)\n",
    "\n",
    "        # remove unneccesary dimensions\n",
    "        resids_pre = resids_pre.squeeze(dim=[0,1])\n",
    "        # assert len(resids_pre) == model.cfg.d_model\n",
    "        assert resids_pre.shape == (model.cfg.d_model,)\n",
    "\n",
    "        mean_resids_per_layer.append(resids_pre.detach().clone())\n",
    "\n",
    "    assert len(mean_resids_per_layer) == model.cfg.n_layers\n",
    "\n",
    "    return mean_resids_per_layer\n",
    "\n",
    "def get_steering_vector_per_layer(\n",
    "    model: HookedTransformer,\n",
    "    prompt1: str,\n",
    "    prompt2: str,\n",
    "    verbose: bool,\n",
    "    max_new_tokens: int,\n",
    ") -> tuple[list[torch.Tensor], str, str]:\n",
    "    prompt1_chat_tokenized, prompt1_chat_str = tokenize_prompt(model, prompt1, verbose)\n",
    "    prompt2_chat_tokenized, prompt2_chat_str = tokenize_prompt(model, prompt2, verbose)\n",
    "    output1, cache1, n_tokens_generated1 = generate_output(model, prompt1_chat_str, max_new_tokens)\n",
    "    output2, cache2, n_tokens_generated2 = generate_output(model, prompt2_chat_str, max_new_tokens)\n",
    "    mean_resids_per_layer1 = get_mean_resids_per_layer(model, cache1, n_tokens_generated1, len(prompt1_chat_tokenized))\n",
    "    mean_resids_per_layer2 = get_mean_resids_per_layer(model, cache2, n_tokens_generated2, len(prompt2_chat_tokenized))\n",
    "\n",
    "    steering_vector_per_layer = [r1 - r2 for r1, r2 in zip(mean_resids_per_layer1, mean_resids_per_layer2)] #keep in mind the direction\n",
    "    return steering_vector_per_layer, output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035bc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the follwing question in French: Who was the first president of USA?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the follwing question in English: Who was the first president of USA?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [00:01<00:04,  7.78it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.26it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt1 = 'Answer the follwing question in French: Who was the first president of USA?'\n",
    "prompt2 = 'Answer the follwing question in English: Who was the first president of USA?'\n",
    "# Other examples to try\n",
    "# prompt1 = 'Answer the follwing question in French: Who was the first president of USA?'\n",
    "# prompt2 = 'Answer the follwing question in English: Who was the first president of USA?'\n",
    "# prompt3 = 'Answer the following in English: Who was the first Tsar of Russia?'\n",
    "\n",
    "vector_per_layer, output1_str, output2_str = get_steering_vector_per_layer(\n",
    "    model=model,\n",
    "    prompt1=prompt1,\n",
    "    prompt2=prompt2,\n",
    "    verbose=True,\n",
    "    max_new_tokens=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090e31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the follwing question in French: Who was the first president of USA?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Le premier président des États-Unis a été George Washington.<|im_end|> \n",
      " <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the follwing question in English: Who was the first president of USA?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The first president of the United States was George Washington. He served as the nation's first president from 1789 to 1797, and his leadership played a crucial role in shaping the country's early history and establishing its democratic\n"
     ]
    }
   ],
   "source": [
    "print(output1_str, \"\\n\", output2_str) # just to check the baseline generations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b0edad",
   "metadata": {},
   "source": [
    "### Warning\n",
    "We do still have to experiment a bit to see if the following code is the way to go for conducting the experiments. The code can be used for smaller experiments to check:\n",
    "- if the steering works?\n",
    "- what layers is it most effective on?\n",
    "- what should the ideal coefficient of steering be?\n",
    "- etc..\n",
    "Do understand what the code does and experiment with it a little!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steered_generation(model, prompt, pos, coeff, steering_vector, layer, token_length):\n",
    "    tokens = model.to_tokens(prompt) #tokenize\n",
    "    \n",
    "    def steer_model(value: torch.Tensor, hook: HookPoint) -> torch.Tensor:\n",
    "        value[:, pos, :] += coeff * torch.tensor(steering_vector) #\n",
    "        return value\n",
    "\n",
    "    with model.hooks(fwd_hooks=[(f\"blocks.{layer}.hook_resid_pre\", steer_model)]): \n",
    "        steered_output = model.generate(tokens, max_new_tokens=token_length)\n",
    "        generation =  model.to_string(steered_output)\n",
    "\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_steering_vector(prompt, model, pos, coeff, layer, token_length, steering_vector):\n",
    "    \n",
    "    temp_tensor = steering_vector[layer]\n",
    "\n",
    "    output = steered_generation(model, prompt, pos, coeff, temp_tensor, layer, token_length)\n",
    "    print(output, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7def363",
   "metadata": {},
   "source": [
    "From this line:\n",
    "`steering_vector_per_layer = [r1 - r2 for r1, r2 in zip(mean_resids_per_layer1, mean_resids_per_layer2)]`\n",
    "\n",
    "When the coeffcient is *positive*\n",
    "- We've calculated the steering vector to steer the output *from* the _second prompt_ (or another equivalent prompt of similar style/meaning) *to* the _first prompt_ (or another equivalent prompt of similar style/meaning)...\n",
    "\n",
    "- And from the first to second for a *negative coefficient*\n",
    "\n",
    "We can think of it in this equation\n",
    "\n",
    "$P_1 - P_2 = \\lambda \\cdot V_s$\n",
    "\n",
    "$P_1 = P_2 + \\lambda \\cdot V_s$\n",
    "\n",
    "$P_1 + (-\\lambda) \\cdot V_s = P_2$\n",
    "\n",
    "Do change the `prompt` parameter in the following cell in the `generate_with_steering_vector` function according to the direction of steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd672279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/var/folders/9b/v7bbwprn3l9dyz86rgj2ktmr0000gn/T/ipykernel_21101/1749975884.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  value[:, pos, :] += coeff * torch.tensor(steering_vector)\n",
      "100%|██████████| 50/50 [00:02<00:00, 16.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Answer the follwing question in English: Who was the first president of USA? Qui était le premier président des USA ?\\nLe premier président des USA est George Washington. George Washington est le premier president of l'USA, born en 1786, a assumed office en 1789, itinerant across\"] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_with_steering_vector(prompt2, model, pos=-1, coeff=1, layer=14, token_length=50, steering_vector=vector_per_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo-neutrality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
