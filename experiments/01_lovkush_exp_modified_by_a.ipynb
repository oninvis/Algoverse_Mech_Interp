{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f51544",
   "metadata": {},
   "source": [
    "# Example by Lovkush of how experimentation might look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf027ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_current_time_str\n",
    "from src.utils import get_repo_root\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDevice():\n",
    "    if torch.cuda.is_available(): #nvidia/runpod\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\") #apple silicon\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "DEVICE = getDevice()\n",
    "DEVICE\n",
    "\n",
    "def get_model(model_name):\n",
    "    # load model from HF and get all the hidden states\n",
    "    model = HookedTransformer.from_pretrained_no_processing(model_name, device = DEVICE, dtype=torch.float16, default_padding_side='left', output_hidden_states=True)\n",
    "    model.eval() #inference mode - no gradients needed\n",
    "    model.to(DEVICE)\n",
    "    return model\n",
    "\n",
    "model = get_model(\"Qwen/Qwen1.5-1.8B-Chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d1758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompt(model: HookedTransformer, prompt_str: str, verbose=False) -> str:\n",
    "    prompt_message = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_str}\n",
    "    ]\n",
    "\n",
    "    if verbose:\n",
    "        print(model.tokenizer.apply_chat_template(\n",
    "            prompt_message,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        ))\n",
    "    prompt_chat_tokenized = model.tokenizer.apply_chat_template(\n",
    "        prompt_message, tokenize=True, add_generation_prompt=True)\n",
    "    prompt_chat_str = model.tokenizer.apply_chat_template(\n",
    "        prompt_message, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt_chat_tokenized, prompt_chat_str\n",
    "\n",
    "def generate_output(model: HookedTransformer, prompt_chat_str: str, max_new_tokens: int) -> tuple[str, dict, int]:\n",
    "    \"\"\"Generate output string, cache, and number of tokens generated.\"\"\"\n",
    "    output_str = prompt_chat_str\n",
    "    for i in tqdm(range(max_new_tokens)):\n",
    "        # Get the logits and cache for the current prompt\n",
    "        logits, cache = model.run_with_cache(output_str)\n",
    "\n",
    "        # Get the predicted next token (using argmax for temperature 0)\n",
    "        next_token = logits[0, -1].argmax()\n",
    "\n",
    "        # Convert the next token to a string\n",
    "        next_token_str = model.to_string(next_token)\n",
    "\n",
    "        # Append the new token to the prompt for the next iteration\n",
    "        output_str += next_token_str\n",
    "        \n",
    "        if next_token.item() == model.tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return output_str, cache, i+1\n",
    "\n",
    "def get_mean_resids_per_layer(model: HookedTransformer, cache: dict, n_tokens_generated: int, n_tokens_input: int) -> list[torch.Tensor]:\n",
    "    mean_resids_per_layer: list[torch.Tensor] = []\n",
    "    n_tokens = n_tokens_generated + n_tokens_input\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        resids_pre = cache[f\"blocks.{layer}.hook_resid_pre\"] # (batch, seq_len, d_model)\n",
    "        assert resids_pre.shape == (1, n_tokens-1, model.cfg.d_model)\n",
    "\n",
    "        # keep only residuals for the generated tokens\n",
    "        resids_pre = resids_pre[:, n_tokens_input:]\n",
    "        assert resids_pre.shape == (1, n_tokens_generated-1, model.cfg.d_model)\n",
    "        \n",
    "        # take the mean across tokens\n",
    "        resids_pre = resids_pre.mean(dim=1, keepdim=True)\n",
    "        assert resids_pre.shape == (1, 1, model.cfg.d_model)\n",
    "\n",
    "        # remove unneccesary dimensions\n",
    "        resids_pre = resids_pre.squeeze(dim=[0,1])\n",
    "        assert len(resids_pre) == (model.cfg.d_model,)\n",
    "\n",
    "        mean_resids_per_layer.append(resids_pre.detach().clone())\n",
    "\n",
    "    assert len(mean_resids_per_layer) == model.cfg.n_layers\n",
    "\n",
    "    return mean_resids_per_layer\n",
    "\n",
    "def get_steering_vector_per_layer(\n",
    "    model: HookedTransformer,\n",
    "    prompt1: str,\n",
    "    prompt2: str,\n",
    "    verbose: bool,\n",
    "    max_new_tokens: int,\n",
    ") -> tuple[list[torch.Tensor], str, str]:\n",
    "    prompt1_chat_tokenized, prompt1_chat_str = tokenize_prompt(model, prompt1, verbose)\n",
    "    prompt2_chat_tokenized, prompt2_chat_str = tokenize_prompt(model, prompt2, verbose)\n",
    "    output1, cache1, n_tokens_generated1 = generate_output(model, prompt1_chat_str, max_new_tokens)\n",
    "    output2, cache2, n_tokens_generated2 = generate_output(model, prompt2_chat_str, max_new_tokens)\n",
    "    mean_resids_per_layer1 = get_mean_resids_per_layer(model, cache1, n_tokens_generated1, len(prompt1_chat_tokenized))\n",
    "    mean_resids_per_layer2 = get_mean_resids_per_layer(model, cache2, n_tokens_generated2, len(prompt2_chat_tokenized))\n",
    "\n",
    "    steering_vector_per_layer = [r1 - r2 for r1, r2 in zip(mean_resids_per_layer1, mean_resids_per_layer2)]\n",
    "    return steering_vector_per_layer, output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = 'If you had to choose, do you prefer sociology or psychology?'\n",
    "prompt2 = 'Do you prefer sociology or psychology?'\n",
    "# Other examples to try\n",
    "# prompt1 = 'Answer the follwing question in French: Who was the first president of USA?'\n",
    "# prompt2 = 'Answer the follwing question in English: Who was the first president of USA?'\n",
    "\n",
    "vector_per_layer, output1_str, output2_str = get_steering_vector_per_layer(\n",
    "    model=model,\n",
    "    prompt1=prompt1,\n",
    "    prompt2=prompt2,\n",
    "    verbose=True,\n",
    "    max_new_tokens=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output1_str, \"\\n\", output2_str) # just to check the baseline generations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf450995",
   "metadata": {},
   "source": [
    "### Warning\n",
    "We do still have to experiment a bit to see if the following code is the way to go for conducting the experiments. The code can be used for smaller experiments to check:\n",
    "- if the steering works?\n",
    "- what layers is it most effective on?\n",
    "- what should the ideal coefficient of steering be?\n",
    "- etc..\n",
    "Do understand what the code does and experiment with it a little!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd672279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steered_generation(model, prompt, pos, coeff, steering_vector, layer, token_length):\n",
    "    tokens = model.to_tokens(prompt) #tokenize\n",
    "    \n",
    "    def steer_model(value: torch.Tensor, hook: HookPoint) -> torch.Tensor:\n",
    "        value[:, pos, :] += coeff * torch.tensor(steering_vector) #\n",
    "        return value\n",
    "\n",
    "    with model.hooks(fwd_hooks=[(f\"blocks.{layer}.hook_resid_pre\", steer_model)]): \n",
    "        steered_output = model.generate(tokens, max_new_tokens=token_length)\n",
    "        generation =  model.to_string(steered_output)\n",
    "\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75732bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_steering_vector(prompt, model, pos, coeff, layer, token_length, steering_vector):\n",
    "    \n",
    "    temp_tensor = steering_vector[layer]\n",
    "\n",
    "    output = steered_generation(model, prompt, pos, coeff, temp_tensor, layer, token_length)\n",
    "    print(output, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfa9651",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = 'Answer the following in English: Who was the first Tsar of Russia?'\n",
    "# Another test case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c67eba9",
   "metadata": {},
   "source": [
    "From this line:\n",
    "`steering_vector_per_layer = [r1 - r2 for r1, r2 in zip(mean_resids_per_layer1, mean_resids_per_layer2)]`\n",
    "\n",
    "When the coeffcient is *positive*\n",
    "- We've calculated the steering vector to steer the output *from* the _second prompt_ (or another equivalent prompt of similar style/meaning) *to* the _first prompt_ (or another equivalent prompt of similar style/meaning)...\n",
    "\n",
    "- And from the first to second for a *negative coefficient*\n",
    "\n",
    "We can think of it in this equation\n",
    "\n",
    "$P_1 - P_2 = \\lambda \\cdot V_s$\n",
    "\n",
    "$P_1 = P_2 + \\lambda \\cdot V_s$\n",
    "\n",
    "$P_1 + (-\\lambda) \\cdot V_s = P_2$\n",
    "\n",
    "Do change the `prompt` parameter in the following cell in the `generate_with_steering_vector` function according to the direction of steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_with_steering_vector(prompt2, model, pos=-1, coeff=1, layer=14, token_length=50, steering_vector=vector_per_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo-neutrality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
