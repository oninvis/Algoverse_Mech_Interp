{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f51544",
   "metadata": {},
   "source": [
    "# Example by Lovkush of how experimentation might look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf027ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_current_time_str\n",
    "from src.utils import get_repo_root\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDevice():\n",
    "    if torch.cuda.is_available(): #nvidia/runpod\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\") #apple silicon\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "DEVICE = getDevice()\n",
    "DEVICE\n",
    "\n",
    "def get_model(model_name):\n",
    "    # load model from HF and get all the hidden states\n",
    "    model = HookedTransformer.from_pretrained_no_processing(model_name, device = DEVICE, dtype=torch.float16, default_padding_side='left', output_hidden_states=True)\n",
    "    model.eval() #inference mode - no gradients needed\n",
    "    model.to(DEVICE)\n",
    "    return model\n",
    "\n",
    "model = get_model(\"Qwen/Qwen1.5-1.8B-Chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d1758",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = 'If you had to choose, do you prefer sociology or psychology?'\n",
    "prompt2 = 'Do you prefer sociology or psychology?'\n",
    "\n",
    "\n",
    "def tokenize_prompt(model: HookedTransformer, prompt_str: str, verbose=False) -> str:\n",
    "    prompt_message = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_str}\n",
    "    ]\n",
    "\n",
    "    if verbose:\n",
    "        print(model.tokenizer.apply_chat_template(\n",
    "            prompt_message,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        ))\n",
    "    prompt_chat_tokenized = model.tokenizer.apply_chat_template(\n",
    "        prompt_message, tokenize=True, add_generation_prompt=True)\n",
    "    prompt_chat_str = model.tokenizer.apply_chat_template(\n",
    "        prompt_message, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt_chat_tokenized, prompt_chat_str\n",
    "\n",
    "def generate_output(model: HookedTransformer, prompt_chat_str: str, max_new_tokens: int) -> tuple[str, dict, int]:\n",
    "    \"\"\"Generate output string, cache, and number of tokens generated.\"\"\"\n",
    "    output_str = prompt_chat_str\n",
    "    for i in tqdm(range(max_new_tokens)):\n",
    "        # Get the logits and cache for the current prompt\n",
    "        logits, cache = model.run_with_cache(output_str)\n",
    "\n",
    "        # Get the predicted next token (using argmax for temperature 0)\n",
    "        next_token = logits[0, -1].argmax()\n",
    "\n",
    "        # Convert the next token to a string\n",
    "        next_token_str = model.to_string(next_token)\n",
    "\n",
    "        # Append the new token to the prompt for the next iteration\n",
    "        output_str += next_token_str\n",
    "        \n",
    "        if next_token.item() == model.tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return output_str, cache, i+1\n",
    "\n",
    "def get_mean_resids_per_layer(model: HookedTransformer, cache: dict, n_tokens_generated: int, n_tokens_input: int) -> list[torch.Tensor]:\n",
    "    mean_resids_per_layer: list[torch.Tensor] = []\n",
    "    n_tokens = n_tokens_generated + n_tokens_input\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        resids_pre = cache[f\"blocks.{layer}.hook_resid_pre\"] # (batch, seq_len, d_model)\n",
    "        assert resids_pre.shape == (1, n_tokens-1, model.cfg.d_model)\n",
    "\n",
    "        # keep only residuals for the generated tokens\n",
    "        resids_pre = resids_pre[:, n_tokens_input:]\n",
    "        assert resids_pre.shape == (1, n_tokens_generated-1, model.cfg.d_model)\n",
    "        \n",
    "        # take the mean across tokens\n",
    "        resids_pre = resids_pre.mean(dim=1, keepdim=True)\n",
    "        assert resids_pre.shape == (1, 1, model.cfg.d_model)\n",
    "\n",
    "        # remove unneccesary dimensions\n",
    "        resids_pre = resids_pre.squeeze(dim=[0,1])\n",
    "        assert len(resids_pre) == model.cfg.d_model\n",
    "\n",
    "        mean_resids_per_layer.append(resids_pre.detach().clone())\n",
    "\n",
    "    assert len(mean_resids_per_layer) == model.cfg.n_layers\n",
    "\n",
    "    return mean_resids_per_layer\n",
    "\n",
    "def get_steering_vector_per_layer(\n",
    "    model: HookedTransformer,\n",
    "    prompt1: str,\n",
    "    prompt2: str,\n",
    "    verbose: bool,\n",
    "    max_new_tokens: int,\n",
    ") -> tuple[list[torch.Tensor], str, str]:\n",
    "    prompt1_chat_tokenized, prompt1_chat_str = tokenize_prompt(model, prompt1, verbose)\n",
    "    prompt2_chat_tokenized, prompt2_chat_str = tokenize_prompt(model, prompt2, verbose)\n",
    "    output1, cache1, n_tokens_generated1 = generate_output(model, prompt1_chat_str, max_new_tokens)\n",
    "    output2, cache2, n_tokens_generated2 = generate_output(model, prompt2_chat_str, max_new_tokens)\n",
    "    mean_resids_per_layer1 = get_mean_resids_per_layer(model, cache1, n_tokens_generated1, len(prompt1_chat_tokenized))\n",
    "    mean_resids_per_layer2 = get_mean_resids_per_layer(model, cache2, n_tokens_generated2, len(prompt2_chat_tokenized))\n",
    "\n",
    "    steering_vector_per_layer = [r1 - r2 for r1, r2 in zip(mean_resids_per_layer1, mean_resids_per_layer2)]\n",
    "    return steering_vector_per_layer, output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = 'If you had to choose, do you prefer sociology or psychology?'\n",
    "prompt2 = 'Do you prefer sociology or psychology?'\n",
    "\n",
    "vector_per_layer, output1_str, output2_str = get_steering_vector_per_layer(\n",
    "    model=model,\n",
    "    prompt1=prompt1,\n",
    "    prompt2=prompt2,\n",
    "    verbose=True,\n",
    "    max_new_tokens=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to write code to do generation with steering hooks. see aryaman's notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd672279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo-neutrality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
