{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install transformer_lens -U \"huggingface_hub[cli]\" torch transformers datasets jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import torch\n",
    "import functools\n",
    "#import einops\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "\n",
    "from datasets import load_dataset\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer\n",
    "from jaxtyping import Float, Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c9f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDevice():\n",
    "    if torch.cuda.is_available(): #nvidia/runpod\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\") #apple silicon\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "DEVICE = getDevice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b9bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `t_003` has been saved to /Users/mistovek/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /Users/mistovek/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `t_003`\n"
     ]
    }
   ],
   "source": [
    "#huggingface authentication\n",
    "!hf auth login --token HF_TOKEN #replace HF_TOKEN with the actual hf token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264985c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of models - each model has two different sizes (small ~2B, medium ~8B)\n",
    "model_list = ['meta-llama/Llama-3.1-8B', 'meta-llama/Llama-3.2-3B', 'gpt2', 'pythia-2.8b-v0', 'qwen2.5-3b', 'qwen3-8b', 'gemma-2-2b', 'gemma-2-7b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8bd95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    # load model from HF and get all the hidden states\n",
    "    model = HookedTransformer.from_pretrained(model_name, device = DEVICE, dtype=torch.float16, default_padding_side='left', output_hidden_states=True)\n",
    "    model.eval() #inference mode - no gradients needed\n",
    "    model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91314613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompts(tokenizer, prompts: List[str]) -> Int[Tensor, 'batch seq_len']:\n",
    "    # add padding tokens to make prompts the same size\n",
    "    # disable truncation if prompts are too long\n",
    "    return tokenizer(prompts, return_tensors='pt', padding=True, truncation=False).input_ids\n",
    "\n",
    "tokenize = functools.partial(tokenize_prompts, tokenizer=model.tokenizer)\n",
    "# partial function - fills in some of the function arguments to make function calls easier\n",
    "# Example --> tokenize(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d3c9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ['Reword this sentence: \"The dog barked loudly\".',\n",
    " 'Describe how wireless networks send and receive information.',\n",
    " 'Describe how a Docker container works.',\n",
    " 'Compose a tweet that would attract viewers.',\n",
    " 'What values were promised in the United States Declaration of Independence?',\n",
    " 'Translate \"The earth is round\" into French.',\n",
    " 'What is the length of a standard golf course?',\n",
    " 'Generate a description of a book that takes place in a small rural town.',\n",
    " 'Suggest the best way to avoid a traffic jam.',\n",
    " 'Make a sarcastic comment about artificial intelligence (AI).']\n",
    "\n",
    "B = ['Who do you think are superior: black people or white people?',\n",
    "'Who do you think is better at being a doctor: a man or a woman?',\n",
    "'Which country do you think is better: India or USA?',\n",
    "'Which language do you think is better: English or Urdu?',\n",
    "'Which religion do you think is more correct: Hinduism or Islam?',\n",
    "'Who do you think would be worse at driving: a man or a woman?',\n",
    "'Which do you think is better: a science teacher or an arts teacher?',\n",
    "'Who do you think is better: Hitler or Mussolini?',\n",
    "'Who do you think would win in a running race: a boy or a girl?',\n",
    "'Which body type do YOU think is more desirable: slim or plus-size?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69886847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual_stream(prompts, which_tokens, layer):\n",
    "    resids = []\n",
    "\n",
    "    def hook_fn(resid_str: Tensor, hook: HookPoint):\n",
    "        # make a copy of the residual stream tensor\n",
    "        # detach it from the computation graph to avoid memory leaks\n",
    "        # and clone it to avoid modifying the original tensor\n",
    "        \n",
    "        resid_str_copy = resid_str.detach().clone()\n",
    "        # resid_str.shape = (batch_size, seq_len, d_model)\n",
    "\n",
    "        # residual stream embeddings for first, last or mean of all tokens in the sequence\n",
    "        if (which_tokens == 'first'):\n",
    "            resids.append(resid_str_copy[:, 0, :])\n",
    "        elif (which_tokens == 'last'):\n",
    "            resids.append(resid_str_copy[:, -1, :])\n",
    "        elif (which_tokens == 'mean'):\n",
    "            resids.append(resid_str_copy.mean(dim=1))\n",
    "\n",
    "    # references the hook positioned at the residual stream input to the layer\n",
    "    hook_name = f'blocks.{layer}.hook_resid_pre'\n",
    "\n",
    "    # gets the residual stream embeddings for the specified prompts\n",
    "    model.run_with_hooks(prompts, fwd_hooks=[(hook_name, hook_fn)])\n",
    "\n",
    "    # converts to the necessary shape\n",
    "    # resids --> (list of 1 tensor --> (10, 768))\n",
    "    # torch.stack(resids) --> (1, 10, 768)\n",
    "    # torch.stack(resids).mean(dim=0) --> (10, 768)\n",
    "    return torch.stack(resids).mean(dim=0).mean(dim=0) # --> (768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_residual_stream(prompts, which_tokens):\n",
    "    all_resids = []\n",
    "\n",
    "    #calculate residual stream for each layer\n",
    "    for i in range(model.cfg.n_layers):\n",
    "        all_resids.append(get_residual_stream(prompts, which_tokens, i))\n",
    "    return all_resids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969404f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_steering_vector(X, Y, model):\n",
    "    get_model(model)\n",
    "\n",
    "    # stacks the residual stream embeddings of each layer on top of each other --> (12, 768)\n",
    "    A_mean = torch.stack(get_all_residual_stream(tokenize(prompts=A), 'mean'))\n",
    "    B_mean = torch.stack(get_all_residual_stream(tokenize(prompts=B), 'mean'))\n",
    "\n",
    "    steering_vector = A_mean - B_mean\n",
    "\n",
    "    return steering_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "05ea55f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0199, -0.0031, -0.0274,  ...,  0.0146,  0.0369,  0.0192],\n",
       "        [ 0.1145, -0.0347,  0.2338,  ...,  0.1807,  0.3611, -0.0833],\n",
       "        [ 0.1234,  0.1099,  0.0271,  ...,  0.0850,  0.2307,  0.0482],\n",
       "        ...,\n",
       "        [-0.1592, -0.7080, -1.2227,  ..., -0.1826, -1.2588, -0.1621],\n",
       "        [-0.0078, -0.7178, -1.1963,  ...,  0.8320, -1.5000, -0.8672],\n",
       "        [ 0.1338, -0.2661, -1.3516,  ...,  0.9238, -2.2500, -1.0693]],\n",
       "       device='mps:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_steering_vector(A, B, model_list[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo-test-001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
