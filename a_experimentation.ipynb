{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4a2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install transformer_lens -U \"huggingface_hub[cli]\" transformers jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78eb2742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mistovek016/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mistovek016/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import functools\n",
    "#import einops\n",
    "import numpy as np\n",
    "#import pandas as pd  \n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer\n",
    "from jaxtyping import Float, Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "317c9f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getDevice():\n",
    "    if torch.cuda.is_available(): #nvidia/runpod\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\") #apple silicon\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "DEVICE = getDevice()\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#huggingface authentication\n",
    "!hf auth login --token HF_TOKEN #replace HF_TOKEN with the actual hf token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264985c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of models - each model has two different sizes (small ~2B, medium ~8B)\n",
    "model_list = ['meta-llama/Llama-3.1-8B', 'meta-llama/Llama-3.2-3B', 'gpt2', 'pythia-2.8b-v0', 'qwen2.5-3b', 'qwen3-8b', 'gemma-2-2b', 'gemma-2-7b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8bd95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    # load model from HF and get all the hidden states\n",
    "    model = HookedTransformer.from_pretrained_no_processing(model_name, device = DEVICE, dtype=torch.float16, default_padding_side='left', output_hidden_states=True)\n",
    "    model.eval() #inference mode - no gradients needed\n",
    "    model.to(DEVICE)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91314613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompts(tokenizer, prompts: List[str]) -> Int[Tensor, 'batch seq_len']:\n",
    "    # add padding tokens to make prompts the same size\n",
    "    # disable truncation if prompts are too long\n",
    "    return tokenizer(prompts, return_tensors='pt', padding=True, truncation=False).input_ids\n",
    "# different tokenizer for chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = []\n",
    "\n",
    "opinionated = []\n",
    "\n",
    "final_dataset = normal + opinionated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d1d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual_stream(prompts, which_tokens, model): #combine methods because of run with cache usage for all layers\n",
    "    resids = torch.empty(len(prompts), 0, model.cfg.d_model).to(DEVICE)\n",
    "    resids_pre = torch.tensor([]).to(DEVICE)\n",
    "    output, cache = model.run_with_cache(prompts) #check if this is model tokens, not prompt tokens\n",
    "    \n",
    "    for i in range(model.cfg.n_layers):\n",
    "\n",
    "        resids_pre = cache[f\"blocks.{i}.hook_resid_pre\"] # (batch, seq_len, d_model)\n",
    "\n",
    "        assert resids_pre.shape == (len(prompts), len(prompts[0]), model.cfg.d_model), f\"Expected shape {(len(prompts), len(prompts[0]), model.cfg.d_model)}, but got {resids_pre.shape}\"\n",
    "        \n",
    "        if (which_tokens == 'first'):\n",
    "            resids_pre = resids_pre[:, 0:1, :]\n",
    "        elif (which_tokens == 'last'):\n",
    "            resids_pre = resids_pre[:, -1:0, :]\n",
    "        elif (which_tokens == 'mean'):\n",
    "            resids_pre = resids_pre.mean(dim=1, keepdim=True)  # mean of all tokens\n",
    "        \n",
    "        assert resids_pre.shape == (len(prompts), 1, model.cfg.d_model), f\"Expected shape {(len(prompts), 1, model.cfg.d_model)}, but got {resids_pre.shape}\"\n",
    "\n",
    "        resids_copy = resids_pre.detach().clone()\n",
    "        resids = torch.cat([resids, resids_copy], dim=1)\n",
    "\n",
    "        assert resids.shape == (len(prompts), i + 1, model.cfg.d_model), f\"Expected shape {(len(prompts), i + 1, model.cfg.d_model)}, but got {resids.shape}\"\n",
    "\n",
    "    resids = resids.mean(dim=0)\n",
    "    assert resids.shape == (model.cfg.n_layers, model.cfg.d_model), f\"Expected shape {(model.cfg.n_layers, model.cfg.d_model)}, but got {resids.shape}\"\n",
    "\n",
    "    return resids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "969404f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_steering_vector(X, Y, model):\n",
    "\n",
    "    # stacks the residual stream embeddings of each layer on top of each other --> (12, 768)\n",
    "    A_mean = get_residual_stream(tokenize_prompts(model.tokenizer, prompts=X), 'mean', model)\n",
    "    B_mean = get_residual_stream(tokenize_prompts(model.tokenizer, prompts=Y), 'mean', model)\n",
    "\n",
    "    steering_vector = A_mean - B_mean\n",
    "\n",
    "    return steering_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = get_model(model_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a0c2869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_llm_judge(prompt):\n",
    "    rand_no = torch.rand(1)\n",
    "    if (rand_no < 0.5): return 0\n",
    "    else: return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e2e0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_prompts(dataset, length):\n",
    "    neutral, opinion = [], []\n",
    "    for i in dataset:\n",
    "        judgement = random_llm_judge(i)\n",
    "        if judgement == 0 and len(neutral) < length: neutral.append(i)\n",
    "        elif judgement == 1 and len(opinion) < length: opinion.append(i)\n",
    "        if len(neutral) >= length and len(opinion) >= length: break\n",
    "    return neutral, opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccf6fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steered_generation(model, prompt, pos, coeff, steering_vector, layer, token_length):\n",
    "    tokens = model.to_tokens(prompt)\n",
    "\n",
    "    def steer_model(value: torch.Tensor, hook: HookPoint) -> torch.Tensor:\n",
    "        value[:, pos, :] += coeff * steering_vector\n",
    "        return value\n",
    "\n",
    "    with model.hooks(fwd_hooks=[(f\"blocks.{layer}.hook_resid_pre\", steer_model)]):\n",
    "        steered_output = model.generate(tokens, max_new_tokens=token_length)\n",
    "        generation =  model.to_string(steered_output)\n",
    "\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47e81e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_generation(model, prompt, token_length):\n",
    "    tokens = model.to_tokens(prompt)\n",
    "\n",
    "    output = model.generate(tokens, max_new_tokens=token_length)\n",
    "    generation = model.to_string(output)\n",
    "\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08ba902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_steering_vector(dataset, dataset_length, model, pos, coeff, layer, token_length):\n",
    "\n",
    "    A, B = seperate_prompts(dataset, length=dataset_length)\n",
    "\n",
    "    steering_vector = calculate_steering_vector(A, B, model)\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "\n",
    "        temp_tensor = steering_vector[layer:layer+1]\n",
    "\n",
    "        output = steered_generation(current_model, dataset[i], pos, coeff, temp_tensor, layer, token_length)\n",
    "        print(f\"Prompt {i + 1}: \", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_with_steering_vector(final_dataset, 2, current_model, 0, 1, 0, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
