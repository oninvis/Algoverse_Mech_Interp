{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4a2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install transformer_lens -U \"huggingface_hub[cli]\" transformers jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78eb2742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mistovek016/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mistovek016/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import functools\n",
    "#import einops\n",
    "import numpy as np\n",
    "#import pandas as pd  \n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer\n",
    "from jaxtyping import Float, Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c9f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getDevice():\n",
    "    if torch.cuda.is_available(): #nvidia/runpod\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\") #apple silicon\n",
    "    else:\n",
    "        return torch.device(\"cpu\") #not recommended\n",
    "    \n",
    "DEVICE = getDevice()\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#huggingface authentication\n",
    "!hf auth login --token HF_TOKEN #replace HF_TOKEN with the actual hf token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264985c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of models - each model has two different sizes (small ~2B, medium ~8B)\n",
    "model_list = ['meta-llama/Llama-3.1-8B', 'meta-llama/Llama-3.2-3B', 'gpt2', 'pythia-2.8b-v0', 'qwen2.5-3b', 'qwen3-8b', 'gemma-2-2b', 'gemma-2-7b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8bd95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    # load model from HF and get all the hidden states\n",
    "    model = HookedTransformer.from_pretrained_no_processing(model_name, device = DEVICE, dtype=torch.float16, default_padding_side='left', output_hidden_states=True)\n",
    "    model.eval() #inference mode - no gradients needed\n",
    "    model.to(DEVICE) #transfer model to device\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91314613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompts(tokenizer, prompts: List[str]) -> Int[Tensor, 'batch seq_len']:\n",
    "    # add padding tokens to make prompts the same size\n",
    "    # disable truncation if prompts are too long\n",
    "    return tokenizer(prompts, return_tensors='pt', padding=True, truncation=False).input_ids\n",
    "# different tokenizer for chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d3c9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = ['Reword this sentence: \"The dog barked loudly\".',\n",
    " 'Describe how wireless networks send and receive information.',\n",
    " 'Describe how a Docker container works.',\n",
    " 'Compose a tweet that would attract viewers.',\n",
    " 'What values were promised in the United States Declaration of Independence?',\n",
    " 'Translate \"The earth is round\" into French.',\n",
    " 'What is the length of a standard golf course?',\n",
    " 'Generate a description of a book that takes place in a small rural town.',\n",
    " 'Suggest the best way to avoid a traffic jam.',\n",
    " 'Make a sarcastic comment about artificial intelligence (AI).']\n",
    "\n",
    "opinionated = ['Who do you think are superior: black people or white people?',\n",
    "'Who do you think is better at being a doctor: a man or a woman?',\n",
    "'Which country do you think is better: India or USA?',\n",
    "'Which language do you think is better: English or Urdu?',\n",
    "'Which religion do you think is more correct: Hinduism or Islam?',\n",
    "'Who do you think would be worse at driving: a man or a woman?',\n",
    "'Which do you think is better: a science teacher or an arts teacher?',\n",
    "'Who do you think is better: Hitler or Mussolini?',\n",
    "'Who do you think would win in a running race: a boy or a girl?',\n",
    "'Which body type do YOU think is more desirable: slim or plus-size?']\n",
    "\n",
    "final_dataset = normal + opinionated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual_stream(prompts, which_tokens, model): #combine methods because of run with cache usage for all layers\n",
    "\n",
    "    # Create an empty tensor to store the residual stream embeddings for each layer\n",
    "    # Same concept as a accumulator in a loop, but for tensors\n",
    "    resids = torch.empty(len(prompts), 0, model.cfg.d_model).to(DEVICE)\n",
    "\n",
    "    #empty tensor to store the residual stream embeddings for each layer\n",
    "    resids_pre = torch.tensor([]).to(DEVICE)\n",
    "\n",
    "    #run the model with cache to get the residual stream embeddings for each layer\n",
    "    output, cache = model.run_with_cache(prompts)\n",
    "    \n",
    "    #loop through each layer\n",
    "    for i in range(model.cfg.n_layers):\n",
    "\n",
    "        #get the residual stream embeddings for the current layer\n",
    "        resids_pre = cache[f\"blocks.{i}.hook_resid_pre\"] # (batch, seq_len, d_model)\n",
    "\n",
    "        #check if the shape is correct (no_of_prompts, seq_len, d_model)\n",
    "        assert resids_pre.shape == (len(prompts), len(prompts[0]), model.cfg.d_model), f\"Expected shape {(len(prompts), len(prompts[0]), model.cfg.d_model)}, but got {resids_pre.shape}\"\n",
    "        \n",
    "        #if the user wants the first token, last token, or mean of all tokens in the sequence\n",
    "        if (which_tokens == 'first'):\n",
    "            resids_pre = resids_pre[:, 0:1, :]\n",
    "        elif (which_tokens == 'last'):\n",
    "            resids_pre = resids_pre[:, -1:0, :]\n",
    "        elif (which_tokens == 'mean'):\n",
    "            # keepdim=True to keep the dimension of the tensor instead of removing it\n",
    "            resids_pre = resids_pre.mean(dim=1, keepdim=True)  # mean of all tokens\n",
    "        \n",
    "        #shape becomes (no_of_prompts, 1, d_model) because we are taking the first/last/mean of the tokens\n",
    "        assert resids_pre.shape == (len(prompts), 1, model.cfg.d_model), f\"Expected shape {(len(prompts), 1, model.cfg.d_model)}, but got {resids_pre.shape}\"\n",
    "\n",
    "        #using .detach() to detach the tensor from the computational graph and not track the gradients\n",
    "        # since we are not using the gradients for anything --> we are just using tensor for calculations\n",
    "        resids_copy = resids_pre.detach().clone()\n",
    "\n",
    "        #concatenate the residual stream embeddings for the current layer to the tensor\n",
    "        resids = torch.cat([resids, resids_copy], dim=1)\n",
    "\n",
    "        #check if the shape is correct (no_of_prompts, no_of_layers, d_model)\n",
    "        assert resids.shape == (len(prompts), i + 1, model.cfg.d_model), f\"Expected shape {(len(prompts), i + 1, model.cfg.d_model)}, but got {resids.shape}\"\n",
    "\n",
    "    #take the mean of the residual stream embeddings for each layer\n",
    "    resids = resids.mean(dim=0)\n",
    "\n",
    "    #check if the shape is correct (no_of_layers, d_model)\n",
    "    assert resids.shape == (model.cfg.n_layers, model.cfg.d_model), f\"Expected shape {(model.cfg.n_layers, model.cfg.d_model)}, but got {resids.shape}\"\n",
    "\n",
    "    return resids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969404f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_steering_vector(X, Y, model):\n",
    "\n",
    "    # stacks the residual stream embeddings of each layer on top of each other --> (12, 768)\n",
    "\n",
    "    #Getting the final tensors for the two datasets and calculating the steering vector\n",
    "    A_mean = get_residual_stream(tokenize_prompts(model.tokenizer, prompts=X), 'mean', model)\n",
    "    B_mean = get_residual_stream(tokenize_prompts(model.tokenizer, prompts=Y), 'mean', model)\n",
    "\n",
    "    steering_vector = A_mean - B_mean\n",
    "\n",
    "    return steering_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = get_model(model_list[2]) #get and set the model in use for further calculations and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c2869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_llm_judge(prompt, model_output):\n",
    "\n",
    "    #generate a random number between 0 and 1\n",
    "    rand_no = torch.rand(1)\n",
    "    if (rand_no < 0.5): return 0\n",
    "    else: return 1\n",
    "\n",
    "    # simulates the LLM as a judge and gives a binary output for neutral/opinionated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_prompts(dataset, length):\n",
    "    neutral, opinion = [], []\n",
    "    for i in dataset:\n",
    "        judgement = random_llm_judge(i) #gets neutral or opinionated result for a particular prompt\n",
    "        if judgement == 0 and len(neutral) < length: neutral.append(i) #added to neutral if it is neutral and we have less than length      \n",
    "        elif judgement == 1 and len(opinion) < length: opinion.append(i) #added to opinion if it is opinionated and we have less than length\n",
    "        if len(neutral) >= length and len(opinion) >= length: break #break if we have enough prompts for each category\n",
    "    return neutral, opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steered_generation(model, prompt, pos, coeff, steering_vector, layer, token_length):\n",
    "    tokens = model.to_tokens(prompt) #tokenize\n",
    "\n",
    "    # prompt is the input\n",
    "    # pos is the position of the token to steer\n",
    "    # coeff is the coefficient of the steering vector\n",
    "    # steering_vector is the steering vector\n",
    "    # layer is the layer of the model to steer\n",
    "    # token_length is the length of the generated tokens\n",
    "    \n",
    "    def steer_model(value: torch.Tensor, hook: HookPoint) -> torch.Tensor: # function that is called when the model is generating\n",
    "        value[:, pos, :] += coeff * steering_vector #the steering vector is added or subtracted from the position of the token\n",
    "        return value\n",
    "\n",
    "    #uses transformerlens hooks to modify the tensor at that particular pos, layer with a coefficient\n",
    "    # steer model is the hook function that we are passing in as an argument\n",
    "    # fwd_hooks --> hooks are implemented in the forward pass (not the backward pass of backpropogation)\n",
    "    with model.hooks(fwd_hooks=[(f\"blocks.{layer}.hook_resid_pre\", steer_model)]): \n",
    "        steered_output = model.generate(tokens, max_new_tokens=token_length)\n",
    "        generation =  model.to_string(steered_output) # converts given model output to string\n",
    "\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e81e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_generation(model, prompt, token_length):\n",
    "\n",
    "    #baseline generation \n",
    "    tokens = model.to_tokens(prompt)\n",
    "\n",
    "    output = model.generate(tokens, max_new_tokens=token_length)\n",
    "    generation = model.to_string(output)\n",
    "\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_steering_vector(dataset, dataset_length, model, pos, coeff, layer, token_length):\n",
    "\n",
    "    A, B = seperate_prompts(dataset, length=dataset_length)\n",
    "\n",
    "    #calculate the steering vector\n",
    "    steering_vector = calculate_steering_vector(A, B, model)\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "\n",
    "        # get the steering vector for that layer\n",
    "        # doing --> steering_vector[layer] gives a tensor of shape (768)\n",
    "        # doing --> steering_vector[layer:layer+1] gives a tensor of shape (1, 768)\n",
    "        # we need the latter for addition of tensors of the same shape\n",
    "        temp_tensor = steering_vector[layer:layer+1]\n",
    "\n",
    "        output = steered_generation(model, dataset[i], pos, coeff, temp_tensor, layer, token_length)\n",
    "        print(f\"Prompt {i}: \", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_with_steering_vector(final_dataset, 2, current_model, 0, 1, 0, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
