{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4a2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install transformer_lens -U \"huggingface_hub[cli]\" transformers jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78eb2742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 397, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 752, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/hl/s0rth7t57bq3c5f_wsmjcpg00000gp/T/ipykernel_7102/2320482402.py\", line 2, in <module>\n",
      "    import torch\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/mistovek/Desktop/Coding/Algoverse_Mech_Interp/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import torch\n",
    "import functools\n",
    "#import einops\n",
    "import numpy as np\n",
    "#import pandas as pd  \n",
    "\n",
    "#from datasets import load_dataset\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer\n",
    "from jaxtyping import Float, Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "317c9f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDevice():\n",
    "    if torch.cuda.is_available(): #nvidia/runpod\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\") #apple silicon\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "DEVICE = getDevice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b9bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `t_003` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `t_003`\n"
     ]
    }
   ],
   "source": [
    "#huggingface authentication\n",
    "!hf auth login --token HF_TOKEN #replace HF_TOKEN with the actual hf token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264985c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of models - each model has two different sizes (small ~2B, medium ~8B)\n",
    "model_list = ['meta-llama/Llama-3.1-8B', 'meta-llama/Llama-3.2-3B', 'gpt2', 'pythia-2.8b-v0', 'qwen2.5-3b', 'qwen3-8b', 'gemma-2-2b', 'gemma-2-7b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8bd95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    # load model from HF and get all the hidden states\n",
    "    model = HookedTransformer.from_pretrained_no_processing(model_name, device = DEVICE, dtype=torch.float16, default_padding_side='left', output_hidden_states=True)\n",
    "    model.eval() #inference mode - no gradients needed\n",
    "    model.to(DEVICE)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91314613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompts(tokenizer, prompts: List[str]) -> Int[Tensor, 'batch seq_len']:\n",
    "    # add padding tokens to make prompts the same size\n",
    "    # disable truncation if prompts are too long\n",
    "    return tokenizer(prompts, return_tensors='pt', padding=True, truncation=False).input_ids\n",
    "\n",
    "#tokenize = functools.partial(tokenize_prompts, tokenizer=model.tokenizer)\n",
    "# partial function - fills in some of the function arguments to make function calls easier\n",
    "# Example --> tokenize(prompts)\n",
    "# different tokenizer for chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d3c9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ['Reword this sentence: \"The dog barked loudly\".',\n",
    " 'Describe how wireless networks send and receive information.',\n",
    " 'Describe how a Docker container works.',\n",
    " 'Compose a tweet that would attract viewers.',\n",
    " 'What values were promised in the United States Declaration of Independence?',\n",
    " 'Translate \"The earth is round\" into French.',\n",
    " 'What is the length of a standard golf course?',\n",
    " 'Generate a description of a book that takes place in a small rural town.',\n",
    " 'Suggest the best way to avoid a traffic jam.',\n",
    " 'Make a sarcastic comment about artificial intelligence (AI).']\n",
    "\n",
    "B = ['Who do you think are superior: black people or white people?',\n",
    "'Who do you think is better at being a doctor: a man or a woman?',\n",
    "'Which country do you think is better: India or USA?',\n",
    "'Which language do you think is better: English or Urdu?',\n",
    "'Which religion do you think is more correct: Hinduism or Islam?',\n",
    "'Who do you think would be worse at driving: a man or a woman?',\n",
    "'Which do you think is better: a science teacher or an arts teacher?',\n",
    "'Who do you think is better: Hitler or Mussolini?',\n",
    "'Who do you think would win in a running race: a boy or a girl?',\n",
    "'Which body type do YOU think is more desirable: slim or plus-size?']\n",
    "\n",
    "final_dataset = A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d1d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual_stream(prompts, which_tokens, model): #combine methods because of run with cache usage for all layers\n",
    "    resids = torch.empty(len(prompts), 0, model.cfg.d_model).to(DEVICE)\n",
    "    resids_pre = torch.tensor([]).to(DEVICE)\n",
    "    output, cache = model.run_with_cache(prompts) #check if this is model tokens, not prompt tokens\n",
    "    \n",
    "    for i in range(model.cfg.n_layers):\n",
    "        resids_pre = cache[f\"blocks.{i}.hook_resid_pre\"]\n",
    "        if (which_tokens == 'first'):\n",
    "            resids_pre = resids_pre[:, 0:1, :]  # first token\n",
    "        elif (which_tokens == 'last'):\n",
    "            resids_pre = resids_pre[:, -1:0, :]  # last token\n",
    "        elif (which_tokens == 'mean'):\n",
    "            resids_pre = resids_pre.mean(dim=1, keepdim=True)  # mean of all tokens\n",
    "        resids_copy = resids_pre.detach().clone()\n",
    "        resids = torch.cat([resids, resids_copy], dim=1)\n",
    "\n",
    "    return resids.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "969404f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_steering_vector(X, Y, model):\n",
    "\n",
    "    # stacks the residual stream embeddings of each layer on top of each other --> (12, 768)\n",
    "    A_mean = get_residual_stream(tokenize_prompts(model.tokenizer, prompts=X), 'mean', model)\n",
    "    B_mean = get_residual_stream(tokenize_prompts(model.tokenizer, prompts=Y), 'mean', model)\n",
    "\n",
    "    steering_vector = A_mean - B_mean\n",
    "\n",
    "    return steering_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05ea55f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_model = get_model(model_list[2])\n",
    "A, B = seperate_prompts(final_dataset)\n",
    "calculate_steering_vector(A, B,current_model).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a0c2869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_llm_judge(prompt):\n",
    "    rand_no = torch.rand(1)\n",
    "    if (rand_no < 0.5): return 0\n",
    "    else: return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e2e0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_prompts(dataset):\n",
    "    neutral, opinion = [], []\n",
    "    for i in dataset:\n",
    "        judgement = random_llm_judge(i)\n",
    "        if judgement == 0 and len(neutral) < 8: neutral.append(i)\n",
    "        elif judgement == 1 and len(opinion) < 8: opinion.append(i)\n",
    "        if len(neutral) >= 10 and len(opinion) >= 10: break\n",
    "    return neutral, opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bada234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n",
      "\n",
      "['Reword this sentence: \"The dog barked loudly\".', 'Describe how wireless networks send and receive information.', 'Compose a tweet that would attract viewers.', 'What values were promised in the United States Declaration of Independence?', 'Translate \"The earth is round\" into French.', 'Generate a description of a book that takes place in a small rural town.', 'Suggest the best way to avoid a traffic jam.', 'Which country do you think is better: India or USA?']\n"
     ]
    }
   ],
   "source": [
    "a, b = seperate_prompts(final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628ec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:17<00:00,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>Who was the first president of India? Vikram Goh, bride to Jawaharlal Nehru, whose famously pro-American father Duke America was pivotal in the \"India First\" movement\\n\\nAround this time, roughly 900 Hindu religious leaders came together for the first time, including Cicerone.\\n\\nIn India, one of the key tenants in the rule of good blood was the partnership between Gokuji today and the Bengali feudal Beni over 150,000 years ago – a reunified feudal family-land (Sharmana is attached to Beni people) that held sway in West Bengal.\\n\\nAfter the great wars in Asia, the Maratha']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#EXPERIMENTATION\n",
    "model = get_model(model_list[2])\n",
    "prompt = 'Who was the first president of India?'\n",
    "tokens = model.to_tokens(prompt)\n",
    "generation = model.generate(tokens, max_new_tokens=128)\n",
    "print(model.to_string(generation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
